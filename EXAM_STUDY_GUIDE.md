# ğŸ“š HOUSE PRICE PREDICTION - EXAM STUDY GUIDE
## Quick Reference for Cramming

**Time to read through:** 15 minutes  
**Best time to review:** Night before exam

---

## ğŸ¯ ONE-SENTENCE DEFINITIONS (Memorize These)

| Term | Definition |
|------|-----------|
| **Regression** | Predicting continuous numbers (prices, temperatures, etc) |
| **Classification** | Predicting categories (yes/no, A/B/C, etc) |
| **Random Forest** | Multiple decision trees voting together |
| **Overfitting** | Model memorizes training data (fails on new data) |
| **Feature** | Input variable (one of the 6: quality, size, etc) |
| **Target** | Output variable (what we predict: price) |
| **MAE** | Average error amount in dollars |
| **RMSE** | Error accounting for large mistakes more |
| **RÂ²** | Percentage of price variation the model explains |
| **Joblib** | Way to save/load trained models |

---

## ğŸ“Š THIS PROJECT IN 30 SECONDS

**What:** Predict house prices using machine learning  
**How:** Train Random Forest on 6 features, deploy to web  
**Why:** Demonstrate ML regression, data preprocessing, web deployment  
**Result:** ~85% accuracy (RÂ² = 0.85)  

---

## ğŸ”¢ THE 6 FEATURES (Know These!)

```
1. OverallQual (1-10)        â†’ Quality rating
2. GrLivArea (sq ft)         â†’ Living space size
3. TotalBsmtSF (sq ft)       â†’ Basement space
4. GarageCars (0-5)          â†’ How many cars fit
5. YearBuilt (1872-2010)     â†’ Age of house
6. FullBath (0-5)            â†’ Full bathrooms
```

**Why these 6?**
- Highest correlation with price
- Least missing data
- Easy to measure and understand

---

## ğŸŒ³ RANDOM FOREST ALGORITHM (Exam Favorite!)

### How It Works:
```
1. Train 100 decision trees
2. Each tree makes a prediction
3. Average all predictions = final answer
```

### Why It's Good:
âœ… Handles complex patterns  
âœ… Doesn't need feature scaling  
âœ… Resistant to outliers  
âœ… Shows feature importance  

### Hyperparameters We Used:
```python
n_estimators=100      # 100 trees
max_depth=20          # Each tree max 20 levels
min_samples_split=5   # Need 5 samples to split node
random_state=42       # Same results every time
```

### Decision Tree Basics:
```
                [OverallQual > 7?]
                     /          \
                  YES            NO
                  /                \
        [Area > 2000?]      [YearBuilt > 2000?]
           /      \              /        \
         ...     ...            ...      ...
```

---

## ğŸ“ˆ FOUR METRICS YOU NEED TO KNOW

### 1. MAE (Mean Absolute Error)
```
Formula: MAE = Average of |actual - predicted|
Units: Dollars ($)
Example: MAE = $22,000 means average prediction off by $22k
Best For: Easy to understand
```

### 2. MSE (Mean Squared Error)
```
Formula: MSE = Average of (actual - predicted)Â²
Units: Dollars squared ($Â²)
Why Squared: Penalizes big errors more than small ones
Problem: Hard to interpret (wrong units)
```

### 3. RMSE (Root Mean Squared Error) â­
```
Formula: RMSE = âˆšMSE
Units: Dollars ($) - same as target!
Example: RMSE = $30,000 is typical error
Why: Best of both worlds - penalizes big errors + easy to interpret
```

### 4. RÂ² (Coefficient of Determination) â­â­
```
Formula: RÂ² = 1 - (SS_residual / SS_total)
Range: 0 to 1 (higher is better)
Example: RÂ² = 0.85 means model explains 85% of price variation
Interpretation:
  - 0.85 = Good (explains 85%)
  - 0.50 = Mediocre (explains 50%)
  - 0.30 = Poor (explains 30%)
```

---

## ğŸ”§ DATA PREPROCESSING STEPS (In Order)

```
1. LOAD DATA
   â””â”€ Read CSV file

2. HANDLE MISSING VALUES
   â””â”€ Fill with median (middle value)

3. SELECT FEATURES
   â””â”€ Choose 6 most important

4. CHECK DATA TYPES
   â””â”€ All should be numbers (int/float)

5. SPLIT DATA
   â””â”€ 80% training, 20% testing

6. TRAIN MODEL
   â””â”€ Fit Random Forest on training data

7. MAKE PREDICTIONS
   â””â”€ Test on unseen test data

8. EVALUATE
   â””â”€ Calculate MAE, MSE, RMSE, RÂ²

9. SAVE MODEL
   â””â”€ Use joblib.dump() to save
```

---

## ğŸ’¾ JOBLIB - MODEL SAVING (For Deployment)

### Why Save Model?
- Don't retrain every time app starts
- 10 minute training becomes instant loading
- Model persists from training to deployment

### Saving (During Training):
```python
import joblib
joblib.dump(trained_model, 'house_price_model.pkl')
```

### Loading (In Web App):
```python
loaded_model = joblib.load('house_price_model.pkl')
prediction = loaded_model.predict(new_data)
```

### Why Joblib over Pickle?
```
Joblib:        Faster, smaller files, safer
Pickle:        Slower, larger files, older method
```

---

## ğŸš€ OVERFITTING VS UNDERFITTING (Common Exam Question)

| Issue | Training Acc | Test Acc | Cause | Solution |
|-------|-------------|----------|-------|----------|
| **Good** | 85% | 85% | Just right | âœ“ Done! |
| **Overfit** | 99% | 70% | Too complex | Reduce depth, more data |
| **Underfit** | 60% | 60% | Too simple | Increase complexity |

### How to Spot Overfitting:
```
IF training_r2 >> test_r2
THEN model is overfitting
```

---

## ğŸ“‹ TRAIN/TEST SPLIT EXPLAINED

```
Total Data: 1460 samples

80% TRAINING (1168 samples)
   â”œâ”€ Model learns patterns
   â”œâ”€ Can see answers
   â””â”€ Typically has high accuracy

20% TESTING (292 samples)
   â”œâ”€ Model has never seen
   â”œâ”€ Can't cheat (doesn't know answers)
   â””â”€ Shows real-world performance

Why 80/20?
- Enough training data to learn
- Enough test data to evaluate fairly
```

---

## ğŸ§  EXAM QUESTIONS & ANSWERS

### Q1: What type of problem is house price prediction?
**A:** Regression (predicting continuous numerical values)

### Q2: Why Random Forest instead of Linear Regression?
**A:** House prices have complex non-linear relationships (area, quality interact in complex ways). Random Forest captures these better.

### Q3: Why 6 features, not all 79?
**A:** 
- More features = more complexity = easier to overfit
- These 6 have highest correlation with price
- Simpler models are easier to understand and deploy

### Q4: What does RÂ² = 0.85 mean?
**A:** The model explains 85% of the variation in house prices. The remaining 15% is due to factors we didn't measure.

### Q5: How do you save a model so you don't retrain?
**A:** Use `joblib.dump(model, 'filename.pkl')`. Load later with `joblib.load('filename.pkl')`.

### Q6: What's the difference between MAE and RMSE?
**A:** 
- MAE = average error (simple)
- RMSE = penalizes large errors more (better for detecting outliers)

### Q7: Explain the Random Forest algorithm in 2 sentences
**A:** It trains multiple decision trees on random subsets of data. Final prediction is the average of all trees' predictions.

### Q8: What's overfitting and how do you detect it?
**A:** Overfitting is when a model memorizes training data instead of learning patterns. You detect it when training accuracy >> test accuracy.

### Q9: Why is feature scaling not needed for Random Forest?
**A:** Random Forest uses decision rules (e.g., "if area > 2000"), not distances. So scale doesn't matter.

### Q10: What preprocessing steps did you do?
**A:** 
1. Handled missing values (filled with median)
2. Selected 6 best features
3. Split into 80% train / 20% test

---

## ğŸ”‘ KEY NUMBERS TO REMEMBER

```
Project Numbers:
- Dataset: 1460 houses, 79 features
- Selected Features: 6
- Train/Test Split: 80/20
- Random Forest Trees: 100
- Expected RÂ²: ~0.85
- Expected RMSE: ~$30,000

Algorithm Numbers:
- Max Tree Depth: 20
- Min Samples Split: 5
- Min Samples Leaf: 2
```

---

## ğŸ“ COMPLETE PIPELINE (Flow)

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   Raw Dataset   â”‚
â”‚   (1460, 79)    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚ Select 6 features
         â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   Clean Data    â”‚
â”‚   (1460, 6)     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚ Fill missing values
         â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   Split Data    â”‚ 80/20
â”‚  Train: 1168    â”‚
â”‚  Test: 292      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚ Train Random Forest
         â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   Trained Model â”‚
â”‚  (100 trees)    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚ Predict on test data
         â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   Evaluate Metrics       â”‚
â”‚  MAE, MSE, RMSE, RÂ²      â”‚
â”‚  RÂ² â‰ˆ 0.85               â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚ Save with joblib
         â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Deploy Web App          â”‚
â”‚  Load model              â”‚
â”‚  Accept user input       â”‚
â”‚  Make predictions        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## âš ï¸ COMMON MISTAKES (Avoid These!)

âŒ **Mistake 1:** Testing on training data
âœ… **Correct:** Always test on new unseen data

âŒ **Mistake 2:** Not handling missing values
âœ… **Correct:** Fill with median or mean

âŒ **Mistake 3:** Using all 79 features
âœ… **Correct:** Select best 6 features

âŒ **Mistake 4:** Not saving the model
âœ… **Correct:** Save with joblib for reuse

âŒ **Mistake 5:** Forgetting to split data
âœ… **Correct:** Always use 80/20 split

---

## ğŸ’¡ QUICK TRICKS FOR EXAM DAY

1. **Remember RÂ²?** Think "R-squared = proportion explained (0-1)"
2. **MAE vs RMSE?** "MAE is average, RMSE penalizes big errors"
3. **Random Forest?** "Multiple trees voting = Random Forest"
4. **Joblib?** "Save model once, load many times"
5. **Overfitting?** "Training > Testing = model memorized"

---

## ğŸ“± CHEAT SHEET

**Quick Formula Reference:**
```
                    Î£D
             MAE = â”€â”€â”€  (D = differences)
                    n

             RMSE = âˆšMSE

                     SS_residual
             RÂ² = 1 - â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
                      SS_total
```

**One-Line Explanations:**
- **Regression:** Predict numbers
- **Random Forest:** Many trees voting
- **MAE:** Average error in dollars
- **RMSE:** Error penalizing big mistakes
- **RÂ²:** Percentage of variation explained
- **Joblib:** Save/load ML models
- **Overfitting:** Memorizes instead of learns
- **Train/Test:** Learn from 80%, evaluate on 20%

---

## â±ï¸ STUDY TIPS

**For 15-minute cramming:**
1. Read this entire guide (12 min)
2. Answer the 10 exam questions (3 min)
3. Review the one-sentence definitions (2 min)

**For 1-hour study:**
1. Read this guide carefully (20 min)
2. Review model_building.ipynb (20 min)
3. Try running the code yourself (20 min)

**For exam day:**
- Skim the one-sentence definitions before exam
- Mentally walk through the pipeline
- Remember: RÂ² â‰ˆ 0.85 (good but not perfect)

---

**Good luck! You've got this! ğŸ“**

---
*Created: January 22, 2026*  
*For: Ogah Victor (22CG031902)*  
*Course: CSC 415 - Covenant University*
